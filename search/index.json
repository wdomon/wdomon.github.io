[{"content":"Be honest, your onprem DNS is probably a bit of a mess. I\u0026rsquo;ve yet to encounter an organization that doesn\u0026rsquo;t have old/stale records in their onprem Active Directory integrated DNS; even when DNS scavenging is enabled. Sometimes this is a lack of time spent on housekeeping within DNS (raises hand: \u0026ldquo;guilty!\u0026rdquo;), other times it\u0026rsquo;s because nobody is quite sure how long that DNS record has been around. We can use Powershell to identify DNS records Created and/or Modified date to help answer some questions about whether it can be deleted. However, the traditional DNS Powershell cmdlets (Get-DnsServerResourceRecord, etc.) will not output any information regarding Created or Modified. Instead, we will leverage the Active Directory database behind the scenes to get these attributes.\nFinding specific DNS objects By default when the Active Directory Powershell Module is installed/imported (either on a Domain Controller or when RSAT Tools are installed), Powershell recognizes \u0026ldquo;AD:\u0026rdquo; as a valid drive and this can be used to browse the AD database. To see the different drives that Powershell will recognize, run Get-PSDrive. We can use Get-ChildItem to iterate through objects in this \u0026ldquo;drive\u0026rdquo; and output attributes of those objects; including the Created and Modified attributes. Typically, Get-ChildItem is used in conjuction with File Systems (network drives, local drives, etc.) so under common cirumstances I would always recommend using the -Filter parameter for that cmdlet when trying to grab specific objects; this cuts WAY down on the amount of time it takes an object to be returned. However, per Microsoft Documentation for that paramater, \u0026ldquo;The FileSystem provider is the only installed Powershell provider that supports filters.\u0026rdquo; As you can see below, our AD: drive does not have a Provider of FileSystem, so we cannot use the -Filter parameter for this. My DNS environment has a little over 11,000 records so it\u0026rsquo;s not a huge loss, but for extremely large environments it could hurt. Assuming that we\u0026rsquo;re not looking to output all records (scroll down for more on that), we can leverage Where-Object to only output the objects based on the criteria we want. For this, we specify the path of our Get-ChildItem function to the DNS path inside the aforementioned AD \u0026ldquo;drive,\u0026rdquo; pipe that to Where-Object to specify the criteria we\u0026rsquo;re looking for (in this case \u0026ldquo;Name\u0026rdquo;), then pipe that to Get-ADObject to output the Created and Modified attributes of that object in the AD database.\nGet-ChildItem -Path \u0026#34;AD:DC=domain.com,CN=MicrosoftDNS,CN=System,DC=domain,DC=com\u0026#34; | Where-Object {$_.Name -eq \u0026#34;\u0026lt;record name\u0026gt;\u0026#34;} | Get-ADObject -Properties Created,Modified | Select-Object Name,Created,Modified #or Get-ChildItem -Path \u0026#34;AD:DC=domain.com,CN=MicrosoftDNS,CN=System,DC=domain,DC=com\u0026#34; | Where-Object {$_.Name -like \u0026#34;*\u0026lt;keyword\u0026gt;*\u0026#34;} | Get-ADObject -Properties Created,Modified | Select-Object Name,Created,Modified Output all DNS objects If you want to output the created and/or modified date of every DNS object in the AD environment just remove the Where-Object function from the pipeline. Given how many objects are likely going to be returned, you will likely want to pipe that to Export-Csv to ensure that you have a readable file rather than having it all dump to the Powershell console.\nGet-ChildItem -Path \u0026#34;AD:DC=domain.com,CN=MicrosoftDNS,CN=System,DC=domain,DC=com\u0026#34; | Get-ADObject -Properties Created,Modified | Select-Object Name,Created,Modified | Export-Csv -Path $env:USERPROFILE\\Downloads\\AD_DNS.csv -NoTypeInformation ","date":"2022-09-22T16:06:33-07:00","permalink":"https://adminbraindump.com/post/find-created-modified-date-of-active-directory-dns-object/","title":"Find Created/Modified Date of Active Directory DNS Object"},{"content":"When working with larger datasets in Powershell it can take a while to output the data that you want. One of the first concepts Powershell users learn is the use of the Where-Object function to pipe data in and limit the output to just the data they are looking for. When working with small datasets, this works totally fine and is widely used. One important nuance of this function, though, is that it needs to wait for all of the data to be piped into it first, then it identifies the objects that match the criteria, and then finally outputs those objects. With larger datasets this can end up taking an extremely long time and if we\u0026rsquo;re only looking for a specific object or set of objects we can drastically reduce the time it takes to output what we want if we instead use the -Filter parameter of functions that support it.\nAs an example, you can see below that there are 351,497 items underneath C:\\Windows. If we use Get-ChildItem and were to try to output only a single file named \u0026ldquo;wlidsvcconfig.xml\u0026rdquo; using Where-Object it has to wait for all 351,497 of those objects to be passed through the pipeline before it can output the file we\u0026rsquo;re looking for. If we use -Filter then it the function will only get objects that match the filter criteria and then pass those through the pipeline. There is still computational load to identify the objects that match the criteria, but it is always going to be significantly less than waiting for all of the objects to be returned first and then having Where-Object look at them. Using this example, it takes less than half the time (27.76 seconds versus 66.86 seconds) to accomplish the same thing.\n","date":"2022-09-22T15:16:18-07:00","permalink":"https://adminbraindump.com/post/improve-powershell-runtimes-using-filter-parameter/","title":"Improve Powershell Runtimes Using the Filter Parameter"},{"content":"In light of the recent news that Uber was hacked largely in part due to a highly privileged credentials being stored in a Powershell script in plain text, I figured some admins writing Powershell might be scrambling to finally level up their secret game. There are lots of ways to store your credentials for Powershell scripts (and effectively all of them are better than plain text); each with their own list of pros and cons, but I\u0026rsquo;ll share one way that provides a layered approach to protecting the secrets while also the flexibility to use those secrets on-prem or in the cloud.\nPrerequisites Azure Az Powershell Module Azure subscription Access to create Azure KeyVault and grant access controls to it Access to create AzureAD Service Principals (App registration) Store the actual secrets in Azure KeyVault The actual usernames/passwords needed by the script to perform operations will be stored in Azure KeyVault. This allows for granular/RBAC permissions to be granted to secrets within Azure as well as a way to separate the credentials so scripts do not have access to more than they need. Anytime you are creating a new workload, function, group of tasks, etc. I recommend to put them in their own resource group. This helps organize things but also helps with applying specific Azure Policy to specific resources. In this example, I create a new resource group and then a new KeyVault to be used for these secrets.\nCreate a Service Principal Now that the KeyVault exists we need to limit who can access the secrets inside. For this we will be creating a service principal by way of an Azure App registration. The naming convention you use for app registrations may vary, the important part here is to differentiate between scripts as each script/task will have it\u0026rsquo;s own Service Principal. Doing so ensures that each script has it\u0026rsquo;s own individual authentication mechanism so logs will clearly indicate which process was authenticating. At this point, you may be thinking that it would be convenient to have one KeyVault with all of your credentials in it and then limit Service Principals to only being able to read the credentials they need. Sadly, Microsoft does not provide a way to set permissison on individaul keys or secrets within a KeyVault; only to the KeyVault itself. As a result, we create a separate KeyVault for each script (though we can keep them all in one Resource Group). We do this so if somehow the service principal was ever compromised that it does not hold the keys to the kingdom, only to the couple of secrets it needs to run and only the ability to read that secret, not modify/delete. Does this make it a pain when we rotate/change passwords? Absolutely, but doing things the \u0026ldquo;right\u0026rdquo; way is typically a pain when compared to the alternative.\nCreate a new Service Principal and note the AppId output. Then, create a new KeyVault Access Policy that grants \u0026ldquo;get\u0026rdquo; permissions for secrets in the desired KeyVault using the aforementioned AppId value for the ServicePrincipalName parameter. This value can also be found for existing Service Principals by looking at the AppId or ServicePrincipalName attributes of that object using Get-AzADServicePrincipal cmdlet.\nCreate a Client Secret The Service Principal now has access to the KeyVault, however we still need a way to use (authenticate as) the Service Principal. Azure automatically creates a Client Secret when you create a new Service Principal. However, when you create it via Powershell you are not presented with what that Client Secret is, and by design you can never recover it again. As a result, I make it a habit to just delete the one Azure creates immediately after I create a Service Principal and create a new one. Because Client Secrets cannot be accessed ever again once they\u0026rsquo;re created, it\u0026rsquo;s important that you account for this when building a new one. As you can see in the example below if you just create a new Secret the SecretText attribute can actually be truncated in your Powershell console and you will not be able to get the full value ever again. As a result, if you are building this interactively be sure to either encase the command in parenthesis and call the SecretText attribute, or use Select-Object to expand the property (my preference).\nConnection in Powershell Script Now all that is left is to tell our Powershell script to authenticate into the Azure tenant AS that Service Principal and then access the Secret in that KeyVault. The first step for this is to obtain the Tenant ID if you don\u0026rsquo;t already have it. The easiest way to do this is when connected to Azure via Powershell just run (Get-AzContext).tenant.id and it will return the ID you need. Once you have this, you connect using the method below by using your Service Principal Name (aka AppId, or ClientID) and the Client Secret. The first line shows that I am not connected to the tenant and the last line shows the that I was able to output the secret in plain text after authenticating as the Service Principal.\nWhat\u0026rsquo;s the difference? Great, so we now have the ability to let our Powershell scripts authenticate to Azure KeyVault to obtain passwords and secrets. But as many people tend to point out at this stage, aren\u0026rsquo;t we just putting the \u0026ldquo;Client Secret\u0026rdquo; in plain text in the script? And isn\u0026rsquo;t that basically just a password as well? Yes and yes. While keeping the password itself out of plain text and in Azure KeyVault instead is definitely more secure, it\u0026rsquo;s just a small hurdle for an attacker to jump through to get the password. The second step here is what helps lock this down in a much more secure way: Encrypting the client secret in a local file on the server running the Powershell script. This is done by leveraging the Data Protection API (DPAPI) built into the Windows Server OS. When this API is used to encrypt a file there are two key requirements that must be met that we take advantage of:\nThe decryption request must come from the user account that was used to originally encrypt it. The decryption request must come from the Windows Server OS that was used to originally encrypt it. This means that if an attacker were able identify that this file holds the Client Secret (let\u0026rsquo;s say by locating the Powershell script and seeing a connection string like in the case of Uber\u0026rsquo;s hack), they could not decrypt the file unless they were logged into the server that originally encrypted the file and ALSO logged in as the user that originally encrypted the file. The catch here is that you must run the scheduled task that is launching the script as the same user that you use to generate the encrypted file; for this use a service account that has no permissions to anything on the network other than the file path it writes this encrypted file to and the location of the script the scheduled task is running.\nEncrypting the local file Encrypting data in a local file is is quite easy and only takes a couple of lines. First we take the variable we used earlier that is just a plain text string containing the Client Secret; we convert it TO a secure string and then back FROM a secure string and dump that to a text file.\nUsing/decrypting the local file Rather than go into detail as to how this works, there is a quick writeup from the folks at PDQ on this topic specifically that you can read if you\u0026rsquo;re interested. Once the file exists, that is it! It\u0026rsquo;s encrypted by the Windows DP API and decrypting it in scripts (that run as the same user account that encrypted it) is as easy as just Get-Content -Path \u0026lt;path to file\u0026gt; | ConvertTo-SecureString. As you can see below, if you read the contents of the text file it is just a long string of alphanumeric characters. To make it usable in a script, you just create a credential object using the ClientID and the decrypted Client Secret and use that when connecting to Azure as the Service Principal!\nAs the warning in the below screenshot mentions, Microsoft has a pretty idiotic default setting in the Connect-AzAccount cmdlet in that it saves the secret being used to connect as a service principal\u0026hellip; IN PLAIN TEXT\u0026hellip; in a text file for \u0026ldquo;convenience.\u0026rdquo; I could go on a tirade about why this shouldn\u0026rsquo;t even exist as an option in this first party cmdlet, but to have it function this way by default is abhorrent. Nonetheless, be sure to run Disable-AzContextAutosave as the service account you use to run the scripts and decrypt the encrypted secret file. Alternatively, you can add Disable-AzContextAutosave -Scope Process into your script prior to connecting to ensure it never writes the file even if it\u0026rsquo;s moved to another server, the module is updated, etc.\n","date":"2022-09-17T08:51:50-07:00","permalink":"https://adminbraindump.com/post/protect-passwords-in-powershell-scripts/","title":"Protect Passwords in Powershell Scripts"},{"content":"If you use a wildcard certificate on your onprem Exchange server, it is common that the CSR and certificate are generated outside of the Exchange environment and you are importing it into Exchange. If that\u0026rsquo;s the case, you don\u0026rsquo;t get an option for changing the Friendly Name of the certificate during the import process. This can result in Exchange listing multiple certificates with the same name on the Certificates page in ECP. Changing friendly name You would think that Microsoft would provide an intuitive way to change the friendly name of a certificate either within the ECP web GUI or at least through Powershell cmdlet. Unfortunately, Set-ExchangeCertificate does not exist and ECP does not give any options to modify the friendly name. Luckily, there is a quick and easy (albeit a bit unintutiive) way to change the friendly name of an Exchange certificate by leveraging a cmdlet that you wouldn\u0026rsquo;t expect to permanently change any attribute: Get-ExchangeCertificate. To do this, just access the FriendlyName attribute of the certificate in question and set/force the name you want in its place; I name my certificates with the Subject Name followed by the expiry year. Alternative method I have come across a couple scenarios where for some reason the above method didn\u0026rsquo;t work. The command wouldn\u0026rsquo;t output any errors, but the certificate just would not have a different friendly name afterwards. In those few examples, I used a slightly different method to accomplish the same thing. For this, we\u0026rsquo;re effectively doing the same thing but instead of using Exchange, we\u0026rsquo;re just leveraging Powershell\u0026rsquo;s Get-ChildItem to objectify the certificate we want. Keeping clear friendly names will help differentiate certificates no matter how those certificates are being viewed and avoid confusion for others on your team!\nCode (Get-ExchangeCertificate -Thumbprint \u0026#34;\u0026lt;thumbprint\u0026gt;\u0026#34;).FriendlyName = \u0026#39;*.domain.com_2023\u0026#39; #or (Get-ChildItem -Path \u0026#34;Cert:\\LocalMachine\\my\\\u0026lt;thumbprint\u0026gt;\u0026#34;).FriendlyName = \u0026#39;*.domain.com_2023\u0026#39; ","date":"2022-09-03T17:59:38-07:00","permalink":"https://adminbraindump.com/post/change-friendly-name-on-exchange-certificate/","title":"Change Friendly Name on Exchange Certificate"},{"content":"Sharepoint Online (SPO) management tools provided by Microsoft leave something to be desired, to say the least. The admin center web GUI is missing an enormous amount of functionality and the Powershell module is pretty unintuitive. While I don\u0026rsquo;t use a ton of SPO, I recently migrated a site to a different M365 tenant and had the need to set that site to a Read-Only state for a period of time (to ensure we had something to reference in the event someone reported that something was missing) and then disable access to it entirely. Using the SPO Powershell Module, you can quickly and easily change the LockState attribute of an SPO site.\nFirst connect to SPO using the administrative URL of your M365 tenant using the Connect-SPOService command. The administrtive URL you connect to is always the same format:\nConnect-SPOService -Url https://tenantname-admin.sharepoint.com\nOnce connected, set the LockState property of the site accordingly using the Set-SPOSite command. This property can accept three values:\nUnlocked - Default state, site is available ReadOnly - Site is in a view only state so no changes can be made. A message appears at the top of the site indicating that the administrator is preventing any changes from being made to this site. Permissions of folders can be viewed but whether or not those permissions are inheriting NoAccess - Disables the site entirely. If the tenant has a value for NoAccessRedirectUrl then all traffic will be forward to that URL. If that value is not set, then a 403 error will be returned for all visitors of the site. Connect-SPOService -Url \u0026#34;https://tenantname-admin.sharepoint.com\u0026#34; Set-SPOSite -Identity \u0026#34;https://tenantname.sharepoint.com/sites/Company\u0026#34; -LockState \u0026#34;ReadOnly\u0026#34; ","date":"2022-08-29T16:03:16-07:00","permalink":"https://adminbraindump.com/post/sharepoint-online-site-lock/","title":"Set a Sharepoint Online site to Read-Only or Disabled"},{"content":"Creating a new post using hugo is literally a one-liner. Just make sure to set the path inside the \u0026lsquo;post\u0026rsquo; folder.\nAfter creating the post, update/save the markdown file then commit and push to the Github repo.\n","date":"2022-08-27T18:46:26-07:00","permalink":"https://adminbraindump.com/post/hugo-new-post/","title":"Create a new post in Hugo"},{"content":"If malicious/suspicious user activity (scanning, auth attempts, etc.) is detected on an Azure App Service, blocking the source IP is often the quickest way to prevent further activity. To do so, it\u0026rsquo;s as easy as adding a Network rule to the app service that blocks that IP/range.\nRequirements Azure Az Powershell Module #Backticks for formatting Add-AzWebAppAccessRestrictionRule ` -ResourceGroupName \u0026#34;ResourceGroup\u0026#34; ` -WebAppName \u0026#34;AppName\u0026#34; ` -Name \u0026#34;Ip example rule\u0026#34; ` #Friendly name of rule -Priority 100 ` #If this app is publicly available, make sure this priority is lower than the priority of the 0.0.0.0/0 rule -Action Deny ` -IpAddress 122.133.144.155/32 #Same command one-liner Add-AzWebAppAccessRestrictionRule -ResourceGroupName \u0026#34;ResourceGroup\u0026#34; -WebAppName \u0026#34;AppName\u0026#34; -Name \u0026#34;Ip example rule\u0026#34; -Priority 100 -Action Deny -IpAddress 122.133.144.155/32 Doing things the old way In the Azure web GUI, access the app service in question and click the \u0026ldquo;Networking\u0026rdquo; navigation blade on the left.\nClick \u0026ldquo;Access restriction\u0026rdquo;\nClick \u0026ldquo;Add rule\u0026rdquo;\nGive the rule a name, change the toggle to \u0026ldquo;Deny,\u0026rdquo; set a priority for the rule (if this is a publicly available app service, make sure to set the new rule with a lower priority than the \u0026ldquo;Allow All\u0026rdquo; rule that exists (0.0.0.0/0), add the CIDR notation of the IP address in question, and click the \u0026ldquo;Add rule\u0026rdquo; button to save the changes. All traffic from that IP address will immediately be blocked.\n","date":"2022-08-27T18:01:14-07:00","permalink":"https://adminbraindump.com/post/azure-app-service-block-ip/","title":"Block an IP address on Azure App Service"}]