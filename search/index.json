[{"content":"Update 01/10/24 Microsoft currently has a feature called Multi-tenant Organization (MTO) in public preview that manages the creation, enable/disabling, and deletion of Guest accounts between different tenants. Microsoft advertises an improved collaborative experience when working with users from the other organization but, so far in my experience, while the automatic sync/management of Guest accounts is a plus - the collaborative experience is actually worse once MTO is involved.\nUpdate 03/16/24 While MTO is still in preview, Microsoft has made great strides towards what I would consider a feature that is close to GA-ready. I have now been using MTO to sync roughly 5,000 total users between two tenants and despite some initial rockiness it has been smooth for the better part of 60 days. For end-users to be able to find each other in the GAL has been a widely praised feature and being able to leverage SSO configurations via Enterprise App means to allow users from the other tenant to SSO into a SaaS application now only requires a few minutes of effort rather than configuring SSO from scratch again (or worse, finding that the SaaS vendor only supports a single IdP). At the time of this writing, I would recommend MTO for any M\u0026amp;A scenario where users need to collaborate within MS Teams or access shared SaaS applications.\nOriginal post Over the last decade or so, Microsoft has done a terrible job at addressing needs for Mergers and Acquisitions of medium to large businesses. They have provided almost no tooling and the features that exist often fall short of being production worthy in an M\u0026amp;A scenario. In many of these scenarios, there is often a business need to get the staff of a newly acquired company to access the parent company\u0026rsquo;s existing platforms (HRIS, ERP, etc). For this, using AzureAD Guest accounts can often provide this access without having to complete a full identity migration of those users into the parent tenant.\nAzureAD (aka EntraID) B2B/Guest user accounts are a very powerful and flexible way to grant individuals outside of your primary tenant access to things as if they are within your tenant. A great example of this is Single SignOn (SSO) access to a third party SaaS application that you need contractors, or users from a newly acquired business, to access. Unfortunately (at the time of writing), even using the most modern methods for creating Guest Users like leveraging MS Graph does not allow for many attributes of a user account to be set at the time of creation; even attributes as basic as First Name or Last Name. These attributes are often passed through SSO configurations, like SAML or OIDC, as claims and are required for these Guest Users to be able to authenticate through SSO. Until Microsoft addresses this, the best course of action is to create all of the Guest Accounts and then take another pass through them to apply the necessary attributes. I recently had this scenario play out after my company acquired another and we needed all of their hiring managers and recruiters (about 300 in total) to access our SaaS recruiting platform. In this post, we will go over a script I wrote to handle all of the steps needed to complete this process.\nPrerequisites Microsoft Graph Powershell Module AzureAD (EntraID) subscription Graph API permissions for User.Invite.All,User.ReadWrite.All, and Directory.ReadWrite.All Ingest list of users The first thing I had the other company provide me was a CSV that contains the three key pieces of information we needed to for each user: FirstName, LastName, and EmailAddress. When I received it, I noticed that the FirstName was in all caps but for cleanliness\u0026rsquo; sake I didn\u0026rsquo;t want these Guest Users to have names in all caps; we\u0026rsquo;ll address this later in the script. (If you\u0026rsquo;re ever sending data to be used programmatically, please don\u0026rsquo;t be \u0026ldquo;that guy\u0026rdquo;; make sure the formatting is decent)\nConnect to MS Graph Connecting to MS Graph using the Powershell module is pretty straight forward: Connect-MgGraph. One thing people tend to forget is to identify the permissions ahead of time that you\u0026rsquo;ll need in order to perform the tasks at hand. For this, we will need User.Invite.All,User.ReadWrite.All, and Directory.ReadWrite.All.\nDeclare variables For SSO configurations, I always lock down the AzureAD Enterprise App to only allow authentication for users in a particular Security Group. One thing to note here is that Guest Users can ONLY be added to a cloud security group and CANNOT be added to a security group that has been synced up from OnPrem AD via AzureAD Connect. Adding these Guest Users to this cloud group is what grants them the ability within AzureAD to authenticate against the Enterprise App and SSO. There\u0026rsquo;s a few ways to grab this group using Get-MgGroup but I\u0026rsquo;ve found searching by display name to be the most convenient; in this case the group is \u0026lsquo;ASG_demo_sso_guests.\u0026rsquo; We\u0026rsquo;re setting $i to 0 now so we can use it as a counter to output the total number of Guest Users created.\nCreating the Guest Users Guest user accounts are created by what MS calls an \u0026ldquo;Invitation.\u0026rdquo; The default behavior when creating these is to send out an email invite that the user interacts with to accept. In most cases, this email causes confusion and generates helpdesk calls so I typically suppress it with -SendInvitationMessage:$false. As mentioned above, the CSV that was provided had users\u0026rsquo; first names in all caps but ideally users would have the first letter of each name capitalized. To fix this before creating the users, we leverage the ToTitleCase method of the Get-Culture cmdlet; this will capitalize the first letter lowercase the rest. I know this works well when location is set to US but I have not tested it outside of that. One nuance of ToTitleCase is that it assumes anything that is all caps is an acronym and doesn\u0026rsquo;t change any of the casing. Since the first names in our CSV are all caps, we can get around this by converting the names to all lowercase while pulling them into ToTitleCase. The below foreach loop will confirm that a Guest User does not already exist for the email address then create a Guest User with the email address while converting the casing of the First and Last names to create the DisplayName (backticks used for formatting).\nOptional: While Loop My experience has been that the time it takes for Guest Users to be created and available to make changes to varies pretty widely. I\u0026rsquo;ve seen them be available near-instant and I\u0026rsquo;ve seen them take 5-10 minutes. Personally, I did not include a While loop in my script to avoid it taking an extremely long time to complete but if you want a single repeatable script for creating Guest Users I would recommend one. As an example, here\u0026rsquo;s one that will check for the existance of the Guest User account but, importantly, give up after 60 seconds.\nAdding the additional attributes and other changes Now that the Guest Users exist, adding attributes is relatively simple and straight forward. Since we are making multiple changes to every user on this list (setting FirstName, setting LastName, and adding user to the security group), we will make all the changes as part of a single foreach loop. One thing to note is that while earlier we ensured to only attempt to create a Guest User if one didn\u0026rsquo;t already exist, we want to make these next changes to every Guest User in the CSV, regardless of whether it existed prior. However, we also want to make sure that an output exists for any failures so we are made aware. Granted, since we\u0026rsquo;re using the same CSV there should be no failures, but it\u0026rsquo;s always good to throw in an extra line or two that helps the confidence of the script output.\nPut it all together $csv = Import-Csv -Path \u0026lt;path to csv\u0026gt; Connect-MgGraph -Scopes User.Invite.All,User.ReadWrite.All,Directory.ReadWrite.All $group = Get-MgGroup -Search \u0026#34;DisplayName:ASG_demo_sso_guests\u0026#34; -ConsistencyLevel eventual $i = 0 foreach ($guest in $csv) { $user = Get-MgUser -Search \u0026#34;mail:$($guest.EmailAddress)\u0026#34; -ConsistencyLevel eventual -ea SilentlyContinue if (!($user)) { New-MgInvitation ` -InvitedUserEmailAddress $($guest.EmailAddress) ` -InviteRedirectUrl \u0026#34;https://portal.office.com\u0026#34; ` -InvitedUserDisplayName (Get-Culture).TextInfo.ToTitleCase(\u0026#34;$($guest.FirstName.toLower()) $($guest.LastName.ToLower())\u0026#34;) ` -SendInvitationMessage:$false ` | Out-Null $i++ } Clear-Variable user } Write-Output \u0026#34;$i guest users created\u0026#34; \u0026lt;# OPTIONAL to be incorporated in the foreach loop $date = Get-Date while (($null -eq $GuestUser) -and ((Get-Date) -le $date.AddSeconds(60))) { $GuestUser = Get-MgUser -Search \u0026#34;mail:$($guest.EmailAddress)\u0026#34; -ConsistencyLevel eventual Start-Sleep -Seconds 1 } #\u0026gt; foreach ($guest in $csv) { if ($user) { Clear-Variable user } $user = Get-MgUser -Search \u0026#34;mail:$($guest.EmailAddress)\u0026#34; -ConsistencyLevel eventual if ($user) { Update-MgUser ` -UserId $user.Id ` -GivenName (Get-Culture).TextInfo.ToTitleCase($($guest.FirstName.toLower())) ` -Surname (Get-Culture).TextInfo.ToTitleCase($($guest.LastName.toLower())) New-MgGroupMember -GroupId $group.id -DirectoryObjectId $user.id } else { Write-Output \u0026#34;Unable to find account with email $($guest.EmailAddress)\u0026#34; } } ","date":"2023-08-27T09:00:50-07:00","permalink":"https://adminbraindump.com/post/azuread-guest-users/","title":"Create AzureAD Guest Users with Additional Attributes using MS Graph"},{"content":"Recently, while investigating a \u0026ldquo;Low Free Space on C: Volume\u0026rdquo; alert on one of our servers I took a look at the C:\\Users folder and noticed a user profile folder for a sysadmin that hadn\u0026rsquo;t worked for the company in over 10 years. I mentioned this to a coworker who said, \u0026ldquo;Yeah I used to look at C:\\Users on all the servers and delete that folder if I saw it but I kind of gave up after a while.\u0026rdquo; If you\u0026rsquo;ve tried this you likely have found that it takes Windows quite a while to delete all the system files, etc. inside that profile folder and can often run into weird NTFS permission issues, or the cursed Thumbs.db \u0026ldquo;in use by another program\u0026rdquo; annoyance.\nWhatever the reason you may have for deleting the user profile from a Windows Server (or workstation), deleting that folder is not even fully removing the user profile from the machine, leaving registry keys and other references to that profile strewn about the OS. The better way to do this is to use Powershell to leverage CIM (Common Information Model) and remove the entire profile, which includes that folder, quickly and easily.\nNuke the profile For my use case, I wanted to cycle through our hundreds of servers to remove this old profile (and a couple others that I knew were floating around out there) from every single one. My first step is to gather a list of just the name of all those servers. Every environment is different but the most generic way to grab all Windows Server names out of Active Directory is by filtering the Get-ADComputer cmdlet.\n#Replace \u0026#39;username\u0026#39; below with the name of the target folder in C:\\Users $servers = (Get-ADComputer -Filter {operatingSystem -like \u0026#34;*Windows Server*\u0026#34;}).Name $username = \u0026#34;username\u0026#34; Once we have all server names and our target username, we can take advantage of the fact that the Get-CimInstance cmdlet\u0026rsquo;s parameter -ComputerName accepts an array of strings (which our $servers variable from above will be). As a result, we won\u0026rsquo;t need a foreach loop, or to push this out as jobs, it\u0026rsquo;s a simple one-liner pipeline to do the rest:\nGet-CimInstance -ComputerName $servers -ClassName Win32_UserProfile | Where-Object {$_.localpath -like \u0026#34;*$username\u0026#34;} | Remove-CimInstance That\u0026rsquo;s it! This will go through all servers and remove all references to the user profile, including the profile folder, that match that username. In my environment it took less time for this to run through hundreds of servers than it took for a single user profile folder to be deleted in File Explorer, so in this case doing things the proper way also ended up being the easy way. Win-Win!\n","date":"2023-01-31T07:57:19-07:00","permalink":"https://adminbraindump.com/post/delete-windows-user-profile/","title":"Properly and Fully Delete Windows User Profile"},{"content":"Be honest, your onprem DNS is probably a bit of a mess. I\u0026rsquo;ve yet to encounter an organization that doesn\u0026rsquo;t have old/stale records in their onprem Active Directory integrated DNS; even when DNS scavenging is enabled. Sometimes this is a lack of time spent on housekeeping within DNS (raises hand: \u0026ldquo;guilty!\u0026rdquo;), other times it\u0026rsquo;s because nobody is quite sure how long that DNS record has been around to identify whether it can be deleted or not. We can use Powershell to identify DNS records Created and/or Modified date to help answer some questions about whether it can be deleted. However, the traditional DNS Powershell cmdlets (Get-DnsServerResourceRecord, etc.) will not output any information regarding Created or Modified. Instead, we will leverage the Active Directory database behind the scenes to get these attributes.\nFinding specific DNS objects By default when the Active Directory Powershell Module is installed/imported (either on a Domain Controller or when RSAT Tools are installed), Powershell recognizes \u0026ldquo;AD:\u0026rdquo; as a valid drive and this can be used to browse the AD database. To see the different drives that Powershell will recognize, run Get-PSDrive.\nWe can use Get-ChildItem to iterate through objects in this \u0026ldquo;drive\u0026rdquo; and output attributes of those objects; including the Created and Modified attributes. Typically, Get-ChildItem is used in conjuction with File Systems (network drives, local drives, etc.) so under common cirumstances I would always recommend using the -Filter parameter for that cmdlet when trying to grab specific objects; this cuts WAY down on the amount of time it takes an object to be returned. However, per Microsoft Documentation for that parameter, \u0026ldquo;The FileSystem provider is the only installed Powershell provider that supports filters.\u0026rdquo; As you can see below, our AD: drive does not have a Provider of FileSystem, so we cannot use the -Filter parameter for this. My DNS environment has a little over 11,000 records so it\u0026rsquo;s not a huge loss, but for extremely large environments it could hurt. Assuming that we\u0026rsquo;re not looking to output all records (scroll down for more on that), we can leverage Where-Object to only output the objects based on the criteria we want. For this, we specify the path of our Get-ChildItem function to the DNS path inside the aforementioned AD \u0026ldquo;drive,\u0026rdquo; pipe that to Where-Object to specify the criteria we\u0026rsquo;re looking for (in this case \u0026ldquo;Name\u0026rdquo;), then pipe that to Get-ADObject to output the Created and Modified attributes of that object in the AD database.\nGet-ChildItem -Path \u0026#34;AD:DC=domain.com,CN=MicrosoftDNS,CN=System,DC=domain,DC=com\u0026#34; | Where-Object {$_.Name -eq \u0026#34;\u0026lt;record name\u0026gt;\u0026#34;} | Get-ADObject -Properties Created,Modified | Select-Object Name,Created,Modified #or Get-ChildItem -Path \u0026#34;AD:DC=domain.com,CN=MicrosoftDNS,CN=System,DC=domain,DC=com\u0026#34; | Where-Object {$_.Name -like \u0026#34;*\u0026lt;keyword\u0026gt;*\u0026#34;} | Get-ADObject -Properties Created,Modified | Select-Object Name,Created,Modified Output all DNS objects If you want to output the created and/or modified date of every DNS object in the AD environment just remove the Where-Object function from the pipeline. Given how many objects are likely going to be returned, you will likely want to pipe that to Export-Csv to ensure that you have a readable file rather than having it all dump to the Powershell console.\nGet-ChildItem -Path \u0026#34;AD:DC=domain.com,CN=MicrosoftDNS,CN=System,DC=domain,DC=com\u0026#34; | Get-ADObject -Properties Created,Modified | Select-Object Name,Created,Modified | Export-Csv -Path $env:USERPROFILE\\Downloads\\AD_DNS.csv -NoTypeInformation ","date":"2022-09-22T16:06:33-07:00","permalink":"https://adminbraindump.com/post/find-created-modified-date-of-active-directory-dns-object/","title":"Find Created/Modified Date of Active Directory DNS Object"},{"content":"When working with larger datasets in Powershell it can take a while to output the data that you want. One of the first concepts Powershell users learn is the use of the Where-Object function to pipe data in and limit the output to just the data they are looking for. When working with small datasets, this works totally fine and is widely used. One important nuance of this function, though, is that it needs to wait for all of the data to be piped into it first, then it identifies the objects that match the criteria, and then finally outputs those objects. With larger datasets this can end up taking an extremely long time and if we\u0026rsquo;re only looking for a specific object or set of objects we can drastically reduce the time it takes to output what we want if we instead use the -Filter parameter of functions that support it.\nAs an example, you can see below that there are 351,497 items underneath C:\\Windows. If we use Get-ChildItem and were to try to output only a single file named \u0026ldquo;wlidsvcconfig.xml\u0026rdquo; using Where-Object it has to wait for all 351,497 of those objects to be passed through the pipeline before it can output the file we\u0026rsquo;re looking for. If we use -Filter then the function will only get objects that match the filter criteria and then pass those through the pipeline. There is still computational load to identify the objects that match the criteria, but at scale it is always going to be significantly less than waiting for all of the objects to be returned first and then having Where-Object look at them. Using this example, it takes less than half the time (27.76 seconds versus 66.86 seconds) to accomplish the same thing.\n","date":"2022-09-22T15:16:18-07:00","permalink":"https://adminbraindump.com/post/improve-powershell-runtimes-using-filter-parameter/","title":"Improve Powershell Runtimes Using the Filter Parameter"},{"content":"In light of the recent news that Uber was hacked largely in part due to a highly privileged credentials being stored in a Powershell script in plain text, I figured some admins writing Powershell might be scrambling to finally level up their secret game. There are lots of ways to store your credentials for Powershell scripts (and effectively all of them are better than plain text); each with their own list of pros and cons, but I\u0026rsquo;ll share one way that provides a layered approach to protecting the secrets while also the flexibility to use those secrets on-prem or in the cloud.\nPrerequisites Azure Az Powershell Module Azure subscription Access to create Azure KeyVault and grant access controls to it Access to create AzureAD Service Principals (App registration) Store the actual secrets in Azure KeyVault The actual usernames/passwords needed by the script to perform operations will be stored in Azure KeyVault. This allows for granular/RBAC permissions to be granted to secrets within Azure as well as a way to separate the credentials so scripts do not have access to more than they need. Anytime you are creating a new workload, function, group of tasks, etc. I recommend to put them in their own resource group. This helps organize things but also helps with applying specific Azure Policy to specific resources. In this example, I create a new resource group and then a new KeyVault to be used for these secrets.\nCreate a Service Principal Now that the KeyVault exists we need to limit who can access the secrets inside. For this we will be creating a service principal by way of an Azure App registration. The naming convention you use for app registrations may vary, the important part here is to differentiate between scripts as each script/task will have it\u0026rsquo;s own Service Principal. Doing so ensures that each script has it\u0026rsquo;s own individual authentication mechanism so logs will clearly indicate which process was authenticating. At this point, you may be thinking that it would be convenient to have one KeyVault with all of your credentials in it and then limit Service Principals to only being able to read the credentials they need. Sadly, Microsoft does not provide a way to set permissison on individaul keys or secrets within a KeyVault; only to the KeyVault itself. As a result, we create a separate KeyVault for each script (though we can keep them all in one Resource Group). We do this so if somehow the service principal was ever compromised that it does not hold the keys to the kingdom, only to the couple of secrets it needs to run and only the ability to read that secret, not modify/delete. Does this make it a pain when we rotate/change passwords? Absolutely, but doing things the \u0026ldquo;right\u0026rdquo; way is typically a pain when compared to the alternative.\nCreate a new Service Principal and note the AppId output. Then, create a new KeyVault Access Policy that grants \u0026ldquo;get\u0026rdquo; permissions for secrets in the desired KeyVault using the aforementioned AppId value for the ServicePrincipalName parameter. This value can also be found for existing Service Principals by looking at the AppId or ServicePrincipalName attributes of that object using Get-AzADServicePrincipal cmdlet.\nCreate a Client Secret The Service Principal now has access to the KeyVault, however we still need a way to use (authenticate as) the Service Principal. Azure automatically creates a Client Secret when you create a new Service Principal. However, when you create it via Powershell you are not presented with what that Client Secret is, and by design you can never recover it again. As a result, I make it a habit to just delete the one Azure creates immediately after I create a Service Principal and create a new one. Because Client Secrets cannot be accessed ever again once they\u0026rsquo;re created, it\u0026rsquo;s important that you account for this when building a new one. As you can see in the example below if you just create a new Secret the SecretText attribute can actually be truncated in your Powershell console and you will not be able to get the full value ever again. As a result, if you are building this interactively be sure to either encase the command in parenthesis and call the SecretText attribute, or use Select-Object to expand the property (my preference).\nConnection in Powershell Script Now all that is left is to tell our Powershell script to authenticate into the Azure tenant AS that Service Principal and then access the Secret in that KeyVault. The first step for this is to obtain the Tenant ID if you don\u0026rsquo;t already have it. The easiest way to do this is when connected to Azure via Powershell just run (Get-AzContext).tenant.id and it will return the ID you need. Once you have this, you connect using the method below by using your Service Principal Name (aka AppId, or ClientID) and the Client Secret. The first line shows that I am not connected to the tenant and the last line shows the that I was able to output the secret in plain text after authenticating as the Service Principal.\nWhat\u0026rsquo;s the difference? Great, so we now have the ability to let our Powershell scripts authenticate to Azure KeyVault to obtain passwords and secrets. But as many people tend to point out at this stage, aren\u0026rsquo;t we just putting the \u0026ldquo;Client Secret\u0026rdquo; in plain text in the script? And isn\u0026rsquo;t that basically just a password as well? Yes and yes. While keeping the password itself out of plain text and in Azure KeyVault instead is definitely more secure, it\u0026rsquo;s just a small hurdle for an attacker to jump through to get the password. The second step here is what helps lock this down in a much more secure way: Encrypting the client secret in a local file on the server running the Powershell script. This is done by leveraging the Data Protection API (DPAPI) built into the Windows Server OS. When this API is used to encrypt a file there are two key requirements that must be met to decrypt the file that we take advantage of:\nThe decryption request must come from the user account that was used to originally encrypt it. The decryption request must come from the Windows Server OS that was used to originally encrypt it. This means that if an attacker were able identify that this file holds the Client Secret (let\u0026rsquo;s say by locating the Powershell script and seeing a connection string like in the case of Uber\u0026rsquo;s hack), they could not decrypt the file unless they were logged into the server that originally encrypted the file and ALSO logged in as the user that originally encrypted the file. The catch here is that you must run the scheduled task that is launching the script as the same user that you use to generate the encrypted file; for this use a service account that has no permissions to anything on the network other than the file path it writes this encrypted file to and the location of the script the scheduled task is running.\nEncrypting the local file Encrypting data in a local file is quite easy and only takes a couple of lines. First we take the variable we used earlier that is just a plain text string containing the Client Secret; we convert it TO a secure string and then back FROM a secure string and dump that to a text file.\nUsing/decrypting the local file Rather than go into detail as to how this works, there is a quick writeup from the folks at PDQ on this topic specifically that you can read if you\u0026rsquo;re interested. Once the file exists, that is it! It is encrypted by the Windows DP API and decrypting it in scripts (that run as the same user account that encrypted it) is as easy as just Get-Content -Path \u0026lt;path to file\u0026gt; | ConvertTo-SecureString. As you can see below, if you read the contents of the text file it is just a long string of alphanumeric characters. To make it usable in a script, you just create a credential object using the ClientID and the decrypted Client Secret and use that when connecting to Azure as the Service Principal!\nAs the warning in the below screenshot mentions, Microsoft has a pretty idiotic default setting in the Connect-AzAccount cmdlet in that it saves the secret being used to connect as a service principal\u0026hellip; IN PLAIN TEXT\u0026hellip; in a text file for \u0026ldquo;convenience.\u0026rdquo; I could go on a tirade about why this shouldn\u0026rsquo;t even exist as an option in this first party cmdlet, but to have it function this way by default is abhorrent. Nonetheless, be sure to run Disable-AzContextAutosave as the service account you use to run the scripts and decrypt the encrypted secret file. Alternatively, you can add Disable-AzContextAutosave -Scope Process into your script prior to connecting to ensure it never writes the file even if it\u0026rsquo;s moved to another server, the module is updated, etc.\n","date":"2022-09-17T08:51:50-07:00","permalink":"https://adminbraindump.com/post/protect-passwords-in-powershell-scripts/","title":"Protect Passwords in Powershell Scripts"},{"content":"If you use a wildcard certificate on your onprem Exchange server, it is common that the CSR and certificate are generated outside of the Exchange environment and you are importing it into Exchange. If that\u0026rsquo;s the case, you don\u0026rsquo;t get an option for changing the Friendly Name of the certificate during the import process. This can result in Exchange listing multiple certificates with the same name on the Certificates page in ECP. Changing friendly name You would think that Microsoft would provide an intuitive way to change the friendly name of a certificate either within the ECP web GUI or at least through Powershell cmdlet. Unfortunately, Set-ExchangeCertificate does not exist and ECP does not give any options to modify the friendly name. Luckily, there is a quick and easy (albeit a bit unintutiive) way to change the friendly name of an Exchange certificate by leveraging a cmdlet that you wouldn\u0026rsquo;t expect to permanently change any attribute: Get-ExchangeCertificate. To do this, just access the FriendlyName attribute of the certificate in question and set/force the name you want in its place; I name my certificates with the Subject Name followed by the expiry year. Alternative method I have come across a couple scenarios where for some reason the above method didn\u0026rsquo;t work. The command wouldn\u0026rsquo;t output any errors, but the certificate just would not have a different friendly name afterwards. In those few examples, I used a slightly different method to accomplish the same thing. For this, we\u0026rsquo;re effectively doing the same thing but instead of using Exchange, we\u0026rsquo;re just leveraging Powershell\u0026rsquo;s Get-ChildItem to objectify the certificate we want. #\nKeeping clear friendly names will help differentiate certificates no matter how those certificates are being viewed and avoid confusion for others on your team!\nCode (Get-ExchangeCertificate -Thumbprint \u0026#34;\u0026lt;thumbprint\u0026gt;\u0026#34;).FriendlyName = \u0026#39;*.domain.com_2023\u0026#39; #or (Get-ChildItem -Path \u0026#34;Cert:\\LocalMachine\\my\\\u0026lt;thumbprint\u0026gt;\u0026#34;).FriendlyName = \u0026#39;*.domain.com_2023\u0026#39; ","date":"2022-09-03T17:59:38-07:00","permalink":"https://adminbraindump.com/post/change-friendly-name-on-exchange-certificate/","title":"Change Friendly Name on Exchange Certificate"},{"content":"Sharepoint Online (SPO) management tools provided by Microsoft leave something to be desired, to say the least. The admin center web GUI is missing an enormous amount of functionality and the Powershell module is pretty unintuitive. While I don\u0026rsquo;t use a ton of SPO, I recently migrated a site to a different M365 tenant and had the need to set that site to a Read-Only state for a period of time (to ensure we had something to reference in the event someone reported that something was missing) and then disable access to it entirely. Using the SPO Powershell Module, you can quickly and easily change the LockState attribute of an SPO site.\nFirst connect to SPO using the administrative URL of your M365 tenant using the Connect-SPOService command. The administrtive URL you connect to is always the same format:\nConnect-SPOService -Url https://tenantname-admin.sharepoint.com\nOnce connected, set the LockState property of the site accordingly using the Set-SPOSite command. This property can accept three values:\nUnlocked - Default state, site is available ReadOnly - Site is in a view only state so no changes can be made. A message appears at the top of the site indicating that the administrator is preventing any changes from being made to this site. Permissions of folders can be viewed but whether or not those permissions are inheriting NoAccess - Disables the site entirely. If the tenant has a value for NoAccessRedirectUrl then all traffic will be forward to that URL. If that value is not set, then a 403 error will be returned for all visitors of the site. Connect-SPOService -Url \u0026#34;https://tenantname-admin.sharepoint.com\u0026#34; Set-SPOSite -Identity \u0026#34;https://tenantname.sharepoint.com/sites/Company\u0026#34; -LockState \u0026#34;ReadOnly\u0026#34; ","date":"2022-08-29T16:03:16-07:00","permalink":"https://adminbraindump.com/post/sharepoint-online-site-lock/","title":"Set a Sharepoint Online site to Read-Only or Disabled"},{"content":"Creating a new post using hugo is literally a one-liner. Just make sure to set the path inside the \u0026lsquo;post\u0026rsquo; folder.\nAfter creating the post, update/save the markdown file then commit and push to the Github repo.\n","date":"2022-08-27T18:46:26-07:00","permalink":"https://adminbraindump.com/post/hugo-new-post/","title":"Create a new post in Hugo"},{"content":"If malicious/suspicious user activity (scanning, auth attempts, etc.) is detected on an Azure App Service, blocking the source IP is often the quickest way to prevent further activity. To do so, it\u0026rsquo;s as easy as adding a Network rule to the app service that blocks that IP/range.\nRequirements Azure Az Powershell Module #Backticks for formatting Add-AzWebAppAccessRestrictionRule ` -ResourceGroupName \u0026#34;ResourceGroup\u0026#34; ` -WebAppName \u0026#34;AppName\u0026#34; ` -Name \u0026#34;Ip example rule\u0026#34; ` #Friendly name of rule -Priority 100 ` #If this app is publicly available, make sure this priority is lower than the priority of the 0.0.0.0/0 rule -Action Deny ` -IpAddress 122.133.144.155/32 #Same command one-liner Add-AzWebAppAccessRestrictionRule -ResourceGroupName \u0026#34;ResourceGroup\u0026#34; -WebAppName \u0026#34;AppName\u0026#34; -Name \u0026#34;Ip example rule\u0026#34; -Priority 100 -Action Deny -IpAddress 122.133.144.155/32 Doing things the old way In the Azure web GUI, access the app service in question and click the \u0026ldquo;Networking\u0026rdquo; navigation blade on the left.\nClick \u0026ldquo;Access restriction\u0026rdquo;\nClick \u0026ldquo;Add rule\u0026rdquo;\nGive the rule a name, change the toggle to \u0026ldquo;Deny,\u0026rdquo; set a priority for the rule (if this is a publicly available app service, make sure to set the new rule with a lower priority than the \u0026ldquo;Allow All\u0026rdquo; rule that exists (0.0.0.0/0), add the CIDR notation of the IP address in question, and click the \u0026ldquo;Add rule\u0026rdquo; button to save the changes. All traffic from that IP address will immediately be blocked.\n","date":"2022-08-27T18:01:14-07:00","permalink":"https://adminbraindump.com/post/azure-app-service-block-ip/","title":"Block an IP address on Azure App Service"}]